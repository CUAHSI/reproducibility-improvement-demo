# CZ_ProcessesDriversPredictability

## Overview

This repository contains code and datasets associated with the manuscript
"Intensive management redefines critical zone processes, drivers, and predictability" (Goodwell et al, 2025; _in review_, submitted to PNAS March 2025).

The analysis applies Gaussian Mixture Model clustering to multivariate time-series datasets to define "temporal regimes", followed by Principal Component Analysis to characterize dominant modes of variability in each regime.  We apply information theory metrics to PC projections to determine dominant predictors of each multivariate system.

**Repository contact:** Allison Goodwell at goodwel2@illinois.edu.

## How to run this analysis

Follow these instructions to run this code. More details about the codebase and files are available in the [How to navigate this codebase section](#how-to-navigate-this-codebase).

### Example analysis

In lieu of running the entire analysis, we have also created an example workflow showing how we analyzed stream chemistry at just one of the sites (Monticello). This example is stored in `Example_Notebook_StreamChemistry.ipynb`.

### Running the full analysis

1. **Setup the required software.** Inspect the `scripts/*.py` files and install any missing packages.
1. **Select a case study.** Choose one of the six case studies to build from the following list. This will be referred to as `[CASE STUDY]` throughout the rest of the instructions.

    * FluxBothSitesHourly
    * FluxGCDaily
    * RiverMonticello
    * RiverOrgeval
    * RiverPlynlimon
    * RootSoilNebraska

1. **Gather the input data.** Identify the input data files used in `scripts/DataPrep_[CASE STUDY].py` and ensure they are all available in the appropriate folders. You may need to download and move some of them into the correct locations. See [the input data section of the README](#input-data) for more information. 
1. **Run data prep.** Run the script `scripts/DataPrep_[CASE STUDY].py`. This should have created a new, intermediate dataset called `data_intermediate/ProcessedData_[CASE STUDY].csv`. This now represents analysis-ready data having been standardized, gap-filled, and QAQC-ed. See more in [the data prep code section of the README](#data-preparation-code).
1. **Run the analysis.** Run the script `scripts/Analysis_[CASE STUDY].py`. This may take some time to run but will generate results output as both a dataset and a set of figures. See more in [the data analysis code section of the README](#analysis-code).
1. **Inspect results.** Open the results products and explore them. There should be one dataset (`data_output/[CASE STUDY]_Results.csv`) and several figures (`figures/[CASE STUDY]_*.svg`).
1. **Repeat for another case study.** You can go back to Step 2 and repeat these same steps to complete the analysis for another case study!

## How to navigate this codebase

### Input data

This repository contains some of the input datasets needed to run the code stored under the following 3 folders:

- `data_input/FluxTowers`
- `data_input/River`
- `data_input/RootSoil`

**Input dataset notes**

* Full datasets for the Monticello RL stream chemistry and NEAG, NEPR soil gas concentration datasets will be uploaded upon acceptance of the manuscript. They are not currently available in this repository, which means some of the code will not be runnable until after publication.
* While some of the GC flux tower data is included in this repository, the full set of GC flux tower data is available in a HydroShare resource: https://www.hydroshare.org/resource/0ef3eda3534f44a6bbd65786d57222ea/
* While one US-KON Ameriflux dataset is included in this repository, all US-Kon Ameriflux datasets are available at: https://ameriflux.lbl.gov/sites/siteinfo/US-Kon

### Data preparation code

The data preparation code relies on the input data described above, some of which is not currently available but will be at the time of publication. Ultimately, they process these input datasets into analysis-ready data.

Code files with the pattern `scripts/DataPrep_*.py` generate the processed data that is used for the analysis codes for each case study.  They align variables from multiple sources, do minor gap-filling and outlier removal for some variables, plot data, and drop NaN values.

You can find the outputs of this data preparation code in the `data_intermediate/` folder. Again, these were generated by the `scripts/DataPrep_*.py` code which assembled input data files for each case study.

### Analysis code

This code starts with the outputs of the data preparation step, stored within this repository under `data_intermediate/`. 

The file `scripts/cluster_funcs.py` contains all necessary functions to perform clustering, dimensionality reduction, and IT metrics and produce figures.  This is imported to each "analysis code" as it contains shared functionality across case studies.

Code files with the pattern `scripts/Analysis_*.py` run the analyses for each case study (based on flux tower data, stream solute concentrations, and root-soil gas concentration datasets and meteorological and soil drivers). Each of these produce several figures (deposited in `figures/`) and a CSV file summarizing the results of the information theory analysis (deposited in `data_output/`).

### Results

The data outputs of the analysis code are stored in the repository under a folder called `data_output/` and contain CSV files for each of the case studies.

All visual outputs summarizing results that were generated by the analysis code are stored in the repository under a folder called `figures/` and contain SVG files.
